{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ZibrowskiLab7.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "KOd6oRQWKOXV",
        "YzOhHIaO7K3N",
        "yoPUu9gg7PnJ"
      ],
      "authorship_tag": "ABX9TyMdo8YIxhDRD1rv2iXsZrnr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zibro011/DataScience-Lab7/blob/main/ZibrowskiLab7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ks1wIipjeFV"
      },
      "source": [
        "**INET 4061 Lab 7** <br>\n",
        "**Adelaide Zibrowski** <br>\n",
        "**October 25, 2021**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xWGO4do60Oe"
      },
      "source": [
        "#Overview\n",
        "For this lab, we are analyzing a data set that contains information regarding different bottles of wine. The dataset consists of 178 bottles of wine from 3 different regions of Italy. Using the available data, it is our responsibility to figure out which region of Italy each bottle of wine originated from. We will do so using basic Random Forest, k-Nearest Neighbors, SVC, and Logistic Regression models. Then, we will apply the Bagging, Boosting, and Stacking ensemble learning techniques to those four models. Each ensemble learning technique will be defined in its corresponding section of the lab. While the business problem is to assign each bottle of wine to its region, we will mainly be using this lab to evaluate the accuracy of different models using different learning techniques.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1skqkriE617Y"
      },
      "source": [
        "#Data\n",
        "This dataset is obtained from https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "061Ltk8Z6h94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44a3e4db-659e-40db-a80d-f8cf1ffeb56e"
      },
      "source": [
        "pip install mlxtend\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.7/dist-packages (0.14.0)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from mlxtend) (57.4.0)\n",
            "Requirement already satisfied: pandas>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.1.5)\n",
            "Requirement already satisfied: matplotlib>=1.5.1 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->mlxtend) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->mlxtend) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->mlxtend) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->mlxtend) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=1.5.1->mlxtend) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.17.1->mlxtend) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->mlxtend) (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jxlu5r4Q8K51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a771c11-fe4c-4af9-ec45-076b36998326"
      },
      "source": [
        "pip install xgboost"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHfe1wd08Pe5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab2eecfc-1bb2-40f6-cd6e-6a0131524f20"
      },
      "source": [
        "#conda install -c conda-forge xgboost\n",
        "#conda install -c conda-forge mlxtend\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn import model_selection\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from mlxtend.classifier import StackingCVClassifier\n",
        "from mlxtend.classifier import EnsembleVoteClassifier\n",
        "\n",
        "from xgboost import XGBClassifier\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YdvE_Omr8mUE",
        "outputId": "b6db1d9a-d697-46bf-faed-d90d0e9f0b32"
      },
      "source": [
        "wine = load_wine()\n",
        "# Convert to Pandas Dataframe\n",
        "wine_dataframe = pd.DataFrame(data = np.c_[wine['data'], wine['target']], \n",
        "                             columns = wine['feature_names'] + ['target'])\n",
        "print(wine.DESCR)\n",
        "wine_dataframe.round(4).describe()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. _wine_dataset:\n",
            "\n",
            "Wine recognition dataset\n",
            "------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 178 (50 in each of three classes)\n",
            "    :Number of Attributes: 13 numeric, predictive attributes and the class\n",
            "    :Attribute Information:\n",
            " \t\t- Alcohol\n",
            " \t\t- Malic acid\n",
            " \t\t- Ash\n",
            "\t\t- Alcalinity of ash  \n",
            " \t\t- Magnesium\n",
            "\t\t- Total phenols\n",
            " \t\t- Flavanoids\n",
            " \t\t- Nonflavanoid phenols\n",
            " \t\t- Proanthocyanins\n",
            "\t\t- Color intensity\n",
            " \t\t- Hue\n",
            " \t\t- OD280/OD315 of diluted wines\n",
            " \t\t- Proline\n",
            "\n",
            "    - class:\n",
            "            - class_0\n",
            "            - class_1\n",
            "            - class_2\n",
            "\t\t\n",
            "    :Summary Statistics:\n",
            "    \n",
            "    ============================= ==== ===== ======= =====\n",
            "                                   Min   Max   Mean     SD\n",
            "    ============================= ==== ===== ======= =====\n",
            "    Alcohol:                      11.0  14.8    13.0   0.8\n",
            "    Malic Acid:                   0.74  5.80    2.34  1.12\n",
            "    Ash:                          1.36  3.23    2.36  0.27\n",
            "    Alcalinity of Ash:            10.6  30.0    19.5   3.3\n",
            "    Magnesium:                    70.0 162.0    99.7  14.3\n",
            "    Total Phenols:                0.98  3.88    2.29  0.63\n",
            "    Flavanoids:                   0.34  5.08    2.03  1.00\n",
            "    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\n",
            "    Proanthocyanins:              0.41  3.58    1.59  0.57\n",
            "    Colour Intensity:              1.3  13.0     5.1   2.3\n",
            "    Hue:                          0.48  1.71    0.96  0.23\n",
            "    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\n",
            "    Proline:                       278  1680     746   315\n",
            "    ============================= ==== ===== ======= =====\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\n",
            "    :Creator: R.A. Fisher\n",
            "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
            "    :Date: July, 1988\n",
            "\n",
            "This is a copy of UCI ML Wine recognition datasets.\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
            "\n",
            "The data is the results of a chemical analysis of wines grown in the same\n",
            "region in Italy by three different cultivators. There are thirteen different\n",
            "measurements taken for different constituents found in the three types of\n",
            "wine.\n",
            "\n",
            "Original Owners: \n",
            "\n",
            "Forina, M. et al, PARVUS - \n",
            "An Extendible Package for Data Exploration, Classification and Correlation. \n",
            "Institute of Pharmaceutical and Food Analysis and Technologies,\n",
            "Via Brigata Salerno, 16147 Genoa, Italy.\n",
            "\n",
            "Citation:\n",
            "\n",
            "Lichman, M. (2013). UCI Machine Learning Repository\n",
            "[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\n",
            "School of Information and Computer Science. \n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "  (1) S. Aeberhard, D. Coomans and O. de Vel, \n",
            "  Comparison of Classifiers in High Dimensional Settings, \n",
            "  Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  \n",
            "  Mathematics and Statistics, James Cook University of North Queensland. \n",
            "  (Also submitted to Technometrics). \n",
            "\n",
            "  The data was used with many others for comparing various \n",
            "  classifiers. The classes are separable, though only RDA \n",
            "  has achieved 100% correct classification. \n",
            "  (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \n",
            "  (All results using the leave-one-out technique) \n",
            "\n",
            "  (2) S. Aeberhard, D. Coomans and O. de Vel, \n",
            "  \"THE CLASSIFICATION PERFORMANCE OF RDA\" \n",
            "  Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \n",
            "  Mathematics and Statistics, James Cook University of North Queensland. \n",
            "  (Also submitted to Journal of Chemometrics).\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>alcohol</th>\n",
              "      <th>malic_acid</th>\n",
              "      <th>ash</th>\n",
              "      <th>alcalinity_of_ash</th>\n",
              "      <th>magnesium</th>\n",
              "      <th>total_phenols</th>\n",
              "      <th>flavanoids</th>\n",
              "      <th>nonflavanoid_phenols</th>\n",
              "      <th>proanthocyanins</th>\n",
              "      <th>color_intensity</th>\n",
              "      <th>hue</th>\n",
              "      <th>od280/od315_of_diluted_wines</th>\n",
              "      <th>proline</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>13.000618</td>\n",
              "      <td>2.336348</td>\n",
              "      <td>2.366517</td>\n",
              "      <td>19.494944</td>\n",
              "      <td>99.741573</td>\n",
              "      <td>2.295112</td>\n",
              "      <td>2.029270</td>\n",
              "      <td>0.361854</td>\n",
              "      <td>1.590899</td>\n",
              "      <td>5.058090</td>\n",
              "      <td>0.957449</td>\n",
              "      <td>2.611685</td>\n",
              "      <td>746.893258</td>\n",
              "      <td>0.938202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.811827</td>\n",
              "      <td>1.117146</td>\n",
              "      <td>0.274344</td>\n",
              "      <td>3.339564</td>\n",
              "      <td>14.282484</td>\n",
              "      <td>0.625851</td>\n",
              "      <td>0.998859</td>\n",
              "      <td>0.124453</td>\n",
              "      <td>0.572359</td>\n",
              "      <td>2.318286</td>\n",
              "      <td>0.228572</td>\n",
              "      <td>0.709990</td>\n",
              "      <td>314.907474</td>\n",
              "      <td>0.775035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>11.030000</td>\n",
              "      <td>0.740000</td>\n",
              "      <td>1.360000</td>\n",
              "      <td>10.600000</td>\n",
              "      <td>70.000000</td>\n",
              "      <td>0.980000</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>0.410000</td>\n",
              "      <td>1.280000</td>\n",
              "      <td>0.480000</td>\n",
              "      <td>1.270000</td>\n",
              "      <td>278.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>12.362500</td>\n",
              "      <td>1.602500</td>\n",
              "      <td>2.210000</td>\n",
              "      <td>17.200000</td>\n",
              "      <td>88.000000</td>\n",
              "      <td>1.742500</td>\n",
              "      <td>1.205000</td>\n",
              "      <td>0.270000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>3.220000</td>\n",
              "      <td>0.782500</td>\n",
              "      <td>1.937500</td>\n",
              "      <td>500.500000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>13.050000</td>\n",
              "      <td>1.865000</td>\n",
              "      <td>2.360000</td>\n",
              "      <td>19.500000</td>\n",
              "      <td>98.000000</td>\n",
              "      <td>2.355000</td>\n",
              "      <td>2.135000</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>1.555000</td>\n",
              "      <td>4.690000</td>\n",
              "      <td>0.965000</td>\n",
              "      <td>2.780000</td>\n",
              "      <td>673.500000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>13.677500</td>\n",
              "      <td>3.082500</td>\n",
              "      <td>2.557500</td>\n",
              "      <td>21.500000</td>\n",
              "      <td>107.000000</td>\n",
              "      <td>2.800000</td>\n",
              "      <td>2.875000</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>1.950000</td>\n",
              "      <td>6.200000</td>\n",
              "      <td>1.120000</td>\n",
              "      <td>3.170000</td>\n",
              "      <td>985.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>14.830000</td>\n",
              "      <td>5.800000</td>\n",
              "      <td>3.230000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>162.000000</td>\n",
              "      <td>3.880000</td>\n",
              "      <td>5.080000</td>\n",
              "      <td>0.660000</td>\n",
              "      <td>3.580000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>1.710000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1680.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          alcohol  malic_acid  ...      proline      target\n",
              "count  178.000000  178.000000  ...   178.000000  178.000000\n",
              "mean    13.000618    2.336348  ...   746.893258    0.938202\n",
              "std      0.811827    1.117146  ...   314.907474    0.775035\n",
              "min     11.030000    0.740000  ...   278.000000    0.000000\n",
              "25%     12.362500    1.602500  ...   500.500000    0.000000\n",
              "50%     13.050000    1.865000  ...   673.500000    1.000000\n",
              "75%     13.677500    3.082500  ...   985.000000    2.000000\n",
              "max     14.830000    5.800000  ...  1680.000000    2.000000\n",
              "\n",
              "[8 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9fiZ99763yo"
      },
      "source": [
        "#Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MarsXqPg6-LJ"
      },
      "source": [
        "##Understand the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "vYc3dEjW69B6",
        "outputId": "7ab76b8f-712f-4b60-afcb-a6ed44a791c7"
      },
      "source": [
        "wine_dataframe.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>alcohol</th>\n",
              "      <th>malic_acid</th>\n",
              "      <th>ash</th>\n",
              "      <th>alcalinity_of_ash</th>\n",
              "      <th>magnesium</th>\n",
              "      <th>total_phenols</th>\n",
              "      <th>flavanoids</th>\n",
              "      <th>nonflavanoid_phenols</th>\n",
              "      <th>proanthocyanins</th>\n",
              "      <th>color_intensity</th>\n",
              "      <th>hue</th>\n",
              "      <th>od280/od315_of_diluted_wines</th>\n",
              "      <th>proline</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14.23</td>\n",
              "      <td>1.71</td>\n",
              "      <td>2.43</td>\n",
              "      <td>15.6</td>\n",
              "      <td>127.0</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.06</td>\n",
              "      <td>0.28</td>\n",
              "      <td>2.29</td>\n",
              "      <td>5.64</td>\n",
              "      <td>1.04</td>\n",
              "      <td>3.92</td>\n",
              "      <td>1065.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13.20</td>\n",
              "      <td>1.78</td>\n",
              "      <td>2.14</td>\n",
              "      <td>11.2</td>\n",
              "      <td>100.0</td>\n",
              "      <td>2.65</td>\n",
              "      <td>2.76</td>\n",
              "      <td>0.26</td>\n",
              "      <td>1.28</td>\n",
              "      <td>4.38</td>\n",
              "      <td>1.05</td>\n",
              "      <td>3.40</td>\n",
              "      <td>1050.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13.16</td>\n",
              "      <td>2.36</td>\n",
              "      <td>2.67</td>\n",
              "      <td>18.6</td>\n",
              "      <td>101.0</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.24</td>\n",
              "      <td>0.30</td>\n",
              "      <td>2.81</td>\n",
              "      <td>5.68</td>\n",
              "      <td>1.03</td>\n",
              "      <td>3.17</td>\n",
              "      <td>1185.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>14.37</td>\n",
              "      <td>1.95</td>\n",
              "      <td>2.50</td>\n",
              "      <td>16.8</td>\n",
              "      <td>113.0</td>\n",
              "      <td>3.85</td>\n",
              "      <td>3.49</td>\n",
              "      <td>0.24</td>\n",
              "      <td>2.18</td>\n",
              "      <td>7.80</td>\n",
              "      <td>0.86</td>\n",
              "      <td>3.45</td>\n",
              "      <td>1480.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>13.24</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.87</td>\n",
              "      <td>21.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.39</td>\n",
              "      <td>1.82</td>\n",
              "      <td>4.32</td>\n",
              "      <td>1.04</td>\n",
              "      <td>2.93</td>\n",
              "      <td>735.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   alcohol  malic_acid   ash  ...  od280/od315_of_diluted_wines  proline  target\n",
              "0    14.23        1.71  2.43  ...                          3.92   1065.0     0.0\n",
              "1    13.20        1.78  2.14  ...                          3.40   1050.0     0.0\n",
              "2    13.16        2.36  2.67  ...                          3.17   1185.0     0.0\n",
              "3    14.37        1.95  2.50  ...                          3.45   1480.0     0.0\n",
              "4    13.24        2.59  2.87  ...                          2.93    735.0     0.0\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "l4HO9CWa8yVN",
        "outputId": "3d768f5d-9b8d-4ff1-ef9f-65935d73a949"
      },
      "source": [
        "wine_dataframe.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>alcohol</th>\n",
              "      <th>malic_acid</th>\n",
              "      <th>ash</th>\n",
              "      <th>alcalinity_of_ash</th>\n",
              "      <th>magnesium</th>\n",
              "      <th>total_phenols</th>\n",
              "      <th>flavanoids</th>\n",
              "      <th>nonflavanoid_phenols</th>\n",
              "      <th>proanthocyanins</th>\n",
              "      <th>color_intensity</th>\n",
              "      <th>hue</th>\n",
              "      <th>od280/od315_of_diluted_wines</th>\n",
              "      <th>proline</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>13.71</td>\n",
              "      <td>5.65</td>\n",
              "      <td>2.45</td>\n",
              "      <td>20.5</td>\n",
              "      <td>95.0</td>\n",
              "      <td>1.68</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.52</td>\n",
              "      <td>1.06</td>\n",
              "      <td>7.7</td>\n",
              "      <td>0.64</td>\n",
              "      <td>1.74</td>\n",
              "      <td>740.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>13.40</td>\n",
              "      <td>3.91</td>\n",
              "      <td>2.48</td>\n",
              "      <td>23.0</td>\n",
              "      <td>102.0</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.43</td>\n",
              "      <td>1.41</td>\n",
              "      <td>7.3</td>\n",
              "      <td>0.70</td>\n",
              "      <td>1.56</td>\n",
              "      <td>750.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>13.27</td>\n",
              "      <td>4.28</td>\n",
              "      <td>2.26</td>\n",
              "      <td>20.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>1.59</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.43</td>\n",
              "      <td>1.35</td>\n",
              "      <td>10.2</td>\n",
              "      <td>0.59</td>\n",
              "      <td>1.56</td>\n",
              "      <td>835.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>13.17</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.37</td>\n",
              "      <td>20.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>1.65</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.53</td>\n",
              "      <td>1.46</td>\n",
              "      <td>9.3</td>\n",
              "      <td>0.60</td>\n",
              "      <td>1.62</td>\n",
              "      <td>840.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>14.13</td>\n",
              "      <td>4.10</td>\n",
              "      <td>2.74</td>\n",
              "      <td>24.5</td>\n",
              "      <td>96.0</td>\n",
              "      <td>2.05</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.35</td>\n",
              "      <td>9.2</td>\n",
              "      <td>0.61</td>\n",
              "      <td>1.60</td>\n",
              "      <td>560.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     alcohol  malic_acid   ash  ...  od280/od315_of_diluted_wines  proline  target\n",
              "173    13.71        5.65  2.45  ...                          1.74    740.0     2.0\n",
              "174    13.40        3.91  2.48  ...                          1.56    750.0     2.0\n",
              "175    13.27        4.28  2.26  ...                          1.56    835.0     2.0\n",
              "176    13.17        2.59  2.37  ...                          1.62    840.0     2.0\n",
              "177    14.13        4.10  2.74  ...                          1.60    560.0     2.0\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLWhT6Kq816D",
        "outputId": "baefed90-88be-4294-b3b9-69a5172730ff"
      },
      "source": [
        "wine_dataframe.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(178, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJgywAPM86GS",
        "outputId": "c1cc6a3b-6d06-443b-afa9-e9071aa66b17"
      },
      "source": [
        "wine_dataframe.nunique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "alcohol                         126\n",
              "malic_acid                      133\n",
              "ash                              79\n",
              "alcalinity_of_ash                63\n",
              "magnesium                        53\n",
              "total_phenols                    97\n",
              "flavanoids                      132\n",
              "nonflavanoid_phenols             39\n",
              "proanthocyanins                 101\n",
              "color_intensity                 132\n",
              "hue                              78\n",
              "od280/od315_of_diluted_wines    122\n",
              "proline                         121\n",
              "target                            3\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCC0PiOD99my",
        "outputId": "17d52e17-9306-4e52-c4f2-7ab79a7451ea"
      },
      "source": [
        "wine_dataframe['target'].unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1., 2.])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ko-mcZ80EXhz",
        "outputId": "f9b2f702-9468-45b0-966d-0e4e5d91d85d"
      },
      "source": [
        "class1 = len(wine_dataframe[wine_dataframe['target']== 0.0])\n",
        "class2 = len(wine_dataframe[wine_dataframe['target']== 1.0])\n",
        "class3 = len(wine_dataframe[wine_dataframe['target']== 2.0])\n",
        "\n",
        "print(\"Class 1 Occurances: \" + str(class1))\n",
        "print(\"Class 2 Occurances: \" + str(class2))\n",
        "print(\"Class 3 Occurances: \" + str(class3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 1 Occurances: 59\n",
            "Class 2 Occurances: 71\n",
            "Class 3 Occurances: 48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c6vFekO9Hgq"
      },
      "source": [
        "From the code above, we can see that we have a dataset with 178 rows and 14 columns. The 14 columns are: alcohol, malic_acid, ash, alcalinity_of_ash, magnesium, total_phenols, flavanoids, nonflavanoid_phenols, proanthocyanins, color_intensity, hue, od280/od315_of_diluted_wines, proline, and target. \n",
        "The target variable has 3 possible values: 0, 1, and 2 which each correspond to a different region of Italy. From here on out, I will refer to the regions as 1, 2, and 3 respectively because that is what 0, 1, and 2 correspond to. There are 59 wines from region 1, 71 wines from region 2, and 48 wines from region 3. While this is somewhat unbalanced, I have decided that it is balanced enough to still work fine. However, if I have trouble fitting my models later, I may have to balance the data. I also printed the metrics in terms of mean, max and min in the Data section. However, these numbers are difficult to interpret without a visualization because we are using categorical data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78qUlRbc7AOW"
      },
      "source": [
        "##Clean the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkH7hj1D7I9U",
        "outputId": "845582d2-dc60-4a1c-9246-4aab0464b44f"
      },
      "source": [
        "wine_dataframe.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "alcohol                         0\n",
              "malic_acid                      0\n",
              "ash                             0\n",
              "alcalinity_of_ash               0\n",
              "magnesium                       0\n",
              "total_phenols                   0\n",
              "flavanoids                      0\n",
              "nonflavanoid_phenols            0\n",
              "proanthocyanins                 0\n",
              "color_intensity                 0\n",
              "hue                             0\n",
              "od280/od315_of_diluted_wines    0\n",
              "proline                         0\n",
              "target                          0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unF_ullGB1eD",
        "outputId": "79dc1b6c-597b-4d1f-fa09-702efb36e217"
      },
      "source": [
        "#drop any rows that contain an outlier in any column\n",
        "#here I define an outlier as a value that is more than 3 standard deviations away from the mean\n",
        "col = wine_dataframe.iloc[:,1:]\n",
        "wine_dataframe[(np.abs(stats.zscore(col)) < 3)]\n",
        "wine_dataframe.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(178, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tfq64nawCXSA"
      },
      "source": [
        "We can see that the dataset does not contain any null values or outliers. Similar to Lab 6, we cannot have any outliers because of the data's categorical nature. It is still useful to take a look at our maximum and minimum values to make sure no categories are too spread out. For example, if one category had a minimum of 5 and a maximum of 5000, we could reasonably assume that there were value(s) in the dataset that were incorrect and would need to be discarded. Our proline variable seems to have an oddly large range between maximum and minimum values. Since I did not know what proline meant, I went ahead and did a quick search and found that proline values can be anywhere between 0 and 3400, so our range is reasonable here (https://ojs.openagrar.de/index.php/VITIS/article/view/7459#:~:text=Th%20e%20ranges%20found%20in,3400%20mg%2Fl%20of%20proline)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vg5F3hoM7Jg2"
      },
      "source": [
        "#Models\n",
        "In the creation and analysis of my models, I used information from https://medium.com/@saugata.paul1010/ensemble-learning-bagging-boosting-stacking-and-cascading-classifiers-in-machine-learning-9c66cb271674."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOd6oRQWKOXV"
      },
      "source": [
        "##Base Learners\n",
        "Here we create and assess the accuracy of our base models with no ensemble classifiers, so we have a basis to compare to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZJVNhF9EC1D",
        "outputId": "5e12e4fc-9985-4a07-a0f4-2d5290e85e75"
      },
      "source": [
        "X = wine_dataframe.drop(\"target\", 1)\n",
        "y = wine_dataframe.target\n",
        "\n",
        "RANDOM_SEED = 4061\n",
        "\n",
        "#Base Learners\n",
        "rf_clf = RandomForestClassifier(n_estimators=10, random_state=RANDOM_SEED)\n",
        "knn_clf = KNeighborsClassifier(n_neighbors=2)\n",
        "svc_clf = SVC(C=10000.0, kernel='rbf', random_state=RANDOM_SEED)\n",
        "lr_clf = LogisticRegression(C=20000, penalty='l2', random_state=RANDOM_SEED)\n",
        "\n",
        "classifier_array = [rf_clf, knn_clf, svc_clf, lr_clf]\n",
        "labels = [clf.__class__.__name__ for clf in classifier_array]\n",
        "\n",
        "normal_accuracy = []\n",
        "normal_std = []\n",
        "bagging_accuracy = []\n",
        "bagging_std = []\n",
        "\n",
        "for clf in classifier_array:\n",
        "    cv_scores = cross_val_score(clf, X, y, cv=3, n_jobs=-1)\n",
        "    \n",
        "    normal_accuracy.append(np.round(cv_scores.mean(),4))\n",
        "    normal_std.append(np.round(cv_scores.std(),4))\n",
        "    \n",
        "    print(\"Accuracy: %0.4f (+/- %0.4f) [Normal %s]\" % (cv_scores.mean(), cv_scores.std(), clf.__class__.__name__))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9213 (+/- 0.0424) [Normal RandomForestClassifier]\n",
            "Accuracy: 0.6465 (+/- 0.0662) [Normal KNeighborsClassifier]\n",
            "Accuracy: 0.9273 (+/- 0.0476) [Normal SVC]\n",
            "Accuracy: 0.9108 (+/- 0.0925) [Normal LogisticRegression]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgMatTrVKz5R"
      },
      "source": [
        "As we an see, our Random Forest, SVC, and Logistic Regression models performed reasonably well, but our K Nearest Neighbors model barely performed better than simple chance. Regardless, none of these models would be accurate enough to answer our business question. If these models were being used in the real world, we would not want our models to have too high of accuracies because that could indicate overfitting (future samples would not have accurate predictions because our model memorized this sample so specifically). However, this is a lab problem, and we have acces to the answers to our business problem. Due to this, we can aim for a 100% accuracy because in this case we truly are only using this model for a single sample set, so if our model memorizes this set, that actually works great. If we wanted to use these models in the future on other wines, we would have to take overfitting into consideration. In summary, I will be striving for 100% accuracy in this lab even though that is not usually a good approach in real life."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzOhHIaO7K3N"
      },
      "source": [
        "##Bagging\n",
        "Below we use the bagging ensemble technique to improve our models from above using a maximum sample size of 0.4. Bagging creates subsets of the data by randomly sampling data points using replacement. Next, individual classifiers are trained using those subsets, and all of those predictions are combined to form the final predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpZaw5urXTCo",
        "outputId": "a7a053bf-789f-4968-d21d-f1b6cd6b90f6"
      },
      "source": [
        "for clf in classifier_array:\n",
        "    cv_scores = cross_val_score(clf, X, y, cv=3, n_jobs=-1)\n",
        "    bagging_clf = BaggingClassifier(clf, max_samples=0.4, max_features=3, random_state=RANDOM_SEED)\n",
        "    bagging_scores = cross_val_score(bagging_clf, X, y, cv=3, n_jobs=-1)\n",
        "    \n",
        "    normal_accuracy.append(np.round(cv_scores.mean(),4))\n",
        "    normal_std.append(np.round(cv_scores.std(),4))\n",
        "    \n",
        "    bagging_accuracy.append(np.round(bagging_scores.mean(),4))\n",
        "    bagging_std.append(np.round(bagging_scores.std(),4))\n",
        "    \n",
        "    print(\"Accuracy: %0.4f (+/- %0.4f) [Normal %s]\" % (cv_scores.mean(), cv_scores.std(), clf.__class__.__name__))\n",
        "    print(\"Accuracy: %0.4f (+/- %0.4f) [Bagging %s]\\n\" % (bagging_scores.mean(), bagging_scores.std(), clf.__class__.__name__))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9213 (+/- 0.0424) [Normal RandomForestClassifier]\n",
            "Accuracy: 0.9440 (+/- 0.0205) [Bagging RandomForestClassifier]\n",
            "\n",
            "Accuracy: 0.6465 (+/- 0.0662) [Normal KNeighborsClassifier]\n",
            "Accuracy: 0.9215 (+/- 0.0442) [Bagging KNeighborsClassifier]\n",
            "\n",
            "Accuracy: 0.9273 (+/- 0.0476) [Normal SVC]\n",
            "Accuracy: 0.9441 (+/- 0.0417) [Bagging SVC]\n",
            "\n",
            "Accuracy: 0.9108 (+/- 0.0925) [Normal LogisticRegression]\n",
            "Accuracy: 0.9328 (+/- 0.0270) [Bagging LogisticRegression]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29RZpJ2bYBpp"
      },
      "source": [
        "We can see from our output above that bagging improved the accuracy of all our models. The improvement to the K-Nearest Neighbors classifier was most substantial with about a 27% increase while the other models increased by around 2% each."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoPUu9gg7PnJ"
      },
      "source": [
        "##Boosting\n",
        "Below we use the boosting ensemble technique to improve our models. Boosting strengthens base learners by training them iteratively where each iteration tries to correct the error made in the previous iteration. We will use four forms of boosting in this lab: Ada Boost, Gradient Boost, XG Boost, and Ensemble. Ada Boost splits the variables into data stumps rather than data trees, which contain only one node and two leaves. Then, these stumps are trained and weighted based on accuracy until all the data points have been correctly classified or the maximum iteration level is reached (https://blog.paperspace.com/adaboost-optimizer/). Gradient Boost uses a loss function to minimize loss while adding decision trees one at a time (https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/). XG Boost is a specific model defined within the scikit-learn library. It is basically a more advanced version of Gradient Boost using a convex loss function (https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/). Finally Ensemble or EnsembleVoteClassifier is a function from the MLEXTEND package that combines different model predictions using the concept of majority voting (https://medium.com/@saugata.paul1010/ensemble-learning-bagging-boosting-stacking-and-cascading-classifiers-in-machine-learning-9c66cb271674)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAonOMpUbUwY",
        "outputId": "78bf8543-3b27-4667-c7ed-6406a1e441c5"
      },
      "source": [
        "ada_boost = AdaBoostClassifier(n_estimators=5)\n",
        "grad_boost = GradientBoostingClassifier(n_estimators=10)\n",
        "xgb_boost = XGBClassifier(max_depth=5, learning_rate=0.001)\n",
        "ensemble_clf = EnsembleVoteClassifier(clfs=[ada_boost, grad_boost, xgb_boost], voting='hard')\n",
        "boosting_labels = ['Ada Boost', 'Gradient Boost', 'XG Boost', 'Ensemble']\n",
        "for clf, label in zip([ada_boost, grad_boost, xgb_boost, ensemble_clf], boosting_labels):\n",
        "    scores = cross_val_score(clf, X, y, cv=3, scoring='accuracy')\n",
        "    print(\"Accuracy: {0:.3f}, Variance: (+/-) {1:.3f} [{2}]\".format(scores.mean(), scores.std(), label))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.843, Variance: (+/-) 0.035 [Ada Boost]\n",
            "Accuracy: 0.837, Variance: (+/-) 0.063 [Gradient Boost]\n",
            "Accuracy: 0.871, Variance: (+/-) 0.070 [XG Boost]\n",
            "Accuracy: 0.871, Variance: (+/-) 0.053 [Ensemble]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YI_V5IzUd1x_"
      },
      "source": [
        "We can see above that implementing boosting classifiers actually decreased our accuracy from the bagging classifiers. However, it these models still performed better than the k-Nearest Neighbors Base model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOvi9WHS7Q7e"
      },
      "source": [
        "##Stacked\n",
        "Stacking combines separately trained models sequentially until a meta-classifier is created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7u2UnoaccnGq",
        "outputId": "b28ea41f-d552-452b-ac46-53b51580fc2b"
      },
      "source": [
        "RANDOM_SEED = 0\n",
        "X = wine_dataframe.drop(\"target\", 1)\n",
        "y = wine_dataframe.target\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder_object = LabelEncoder()\n",
        "y = encoder_object.fit_transform(y)\n",
        "#Base Learners\n",
        "rf_clf = RandomForestClassifier(n_estimators=10, random_state=RANDOM_SEED)\n",
        "knn_clf = KNeighborsClassifier(n_neighbors=2)\n",
        "svc_clf = SVC(C=10000.0, kernel='rbf', random_state=RANDOM_SEED)\n",
        "lr_clf = LogisticRegression(C=20000, penalty='l2', random_state=RANDOM_SEED)\n",
        "lr = LogisticRegression(random_state=RANDOM_SEED) # meta classifier\n",
        "lr_clf.max_iter = 100000\n",
        "sclf = StackingClassifier(classifiers=[rf_clf, knn_clf, svc_clf, lr_clf], meta_classifier=lr)\n",
        "classifier_array = [rf_clf, knn_clf, svc_clf, lr_clf, sclf ]\n",
        "labels = [clf.__class__.__name__ for clf in classifier_array]\n",
        "acc_list = []\n",
        "var_list = []\n",
        "for clf, label in zip(classifier_array, labels):\n",
        "    cv_scores = model_selection.cross_val_score(clf, X, y, cv=3, scoring='accuracy')\n",
        "    print(\"Accuracy: %0.4f (+/- %0.4f) [%s]\" % (cv_scores.mean(), cv_scores.std(), label))\n",
        "    acc_list.append(np.round(cv_scores.mean(),4))\n",
        "    var_list.append(np.round(cv_scores.std(),4))\\\n",
        "    #print(\"Accuracy: %0.4f (+/- %0.4f) [%s]\" % (cv_scores.mean(), cv_scores.std(), label))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9046 (+/- 0.0442) [RandomForestClassifier]\n",
            "Accuracy: 0.6465 (+/- 0.0662) [KNeighborsClassifier]\n",
            "Accuracy: 0.9273 (+/- 0.0476) [SVC]\n",
            "Accuracy: 0.9271 (+/- 0.0204) [LogisticRegression]\n",
            "Accuracy: 0.9329 (+/- 0.0407) [StackingClassifier]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nm_tNdRNDxkA"
      },
      "source": [
        "The final stacking model performed better than all of our previous models except Random Forest using Bagging and SVC using Bagging. Regardless, a 93% accuracy is fairly good especially considering one of the included models (k-Nearest Neighbors) had a initial accuracy of just 65%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12H6r3Z47x9e"
      },
      "source": [
        "###Stacking with Grid Search Cross Validation\n",
        "In the words of data scientist Satyam Kumar, \"Grid Search Cross Validation tries all combinations of parameters grid for a model and returns with the best set of parameters having the best performance score\" (https://towardsdatascience.com/20x-times-faster-grid-search-cross-validation-19ef01409b7c)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60qU0p2FhC23",
        "outputId": "ee1afa9a-3501-4484-cc73-14b78d26914c"
      },
      "source": [
        "#Base Learners.\n",
        "rf_clf = RandomForestClassifier(random_state=RANDOM_SEED,n_jobs=-1)\n",
        "knn_clf = KNeighborsClassifier(p=2, metric='minkowski',n_jobs=-1)\n",
        "lr = LogisticRegression(random_state=RANDOM_SEED) # meta classifier\n",
        "#sclf = StackingClassifier(classifiers=[rf_clf, et_clf, knn_clf, svc_clf, rg_clf, lr_clf, dt_clf, adab_clf], meta_classifier=lr)\n",
        "sclf = StackingClassifier(classifiers=[rf_clf, knn_clf], meta_classifier=lr)\n",
        "print(\"\\nAccuracies of all classifiers using grid search cross validation.\")\n",
        "params = {'randomforestclassifier__n_estimators': np.arange(10,20), 'randomforestclassifier__max_depth': np.arange(1,5), \n",
        "          'kneighborsclassifier__n_neighbors': np.arange(1,20,2),\n",
        "          'meta-logisticregression__C': [0.001,0.01,0.1,1,10,100,1000]}\n",
        "gsearch_cv = GridSearchCV(estimator=sclf, param_grid=params, cv=5, refit=True)\n",
        "gsearch_cv.fit(X, y)\n",
        "cv_keys = ('mean_test_score', 'std_test_score', 'params')\n",
        "print('Best parameters: %s' % gsearch_cv.best_params_)\n",
        "print('Accuracy: %.2f' % gsearch_cv.best_score_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracies of all classifiers using grid search cross validation.\n",
            "Best parameters: {'kneighborsclassifier__n_neighbors': 3, 'meta-logisticregression__C': 10, 'randomforestclassifier__max_depth': 4, 'randomforestclassifier__n_estimators': 15}\n",
            "Accuracy: 0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHv60MHZIkQ0"
      },
      "source": [
        "Using Grid Search Cross Validation, our accuracy increased all the way to 98%. This is obviously the most accurate model, but it took 3 hours to compute for me which is the downside to it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVNkQ6REhg6-"
      },
      "source": [
        "#Conclusion\n",
        "After making predictions using different models with different ensemble learning techniques, we can see that Stacking with Grid Search Cross Validation is by far the most accurate method, and the basic k-Nearest Neighbors model is the worst method in terms of accuracy for this specific dataset. If we had the time to run Grid Search Cross Validation, that would be our best option for answering our business question accurately. If we were short on time, however, we might opt to use Random Forest using Bagging or SVC using Bagging because they are the most accurate of the other models and they do not take nearly as long as Grid Search Cross Validation. It makes sense that the k-Nearest Neighbors model is the worst because that model does not do well with unbalanced data. Even though this data is not that unbalanced, it is still too unbalanced for k-Nearest Neighbors to use accurately on its own. It also makes sense that Stacking with Grid Search Cross Validation was the most accurate because the algorithm spent an entire 3 hours testing every possible combination of parameters until it found the one that produced the highest accuracy. "
      ]
    }
  ]
}